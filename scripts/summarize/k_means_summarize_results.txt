{ u'A00-1031': [ [ u' The tagger described in this paper is based on the standard Hidden Markov Model architecture .'],
                 [],
                 [ u'The TNT POS tagger has also been designed to train and run very quickly, tagging between 30,000 and 60,000 words per second.']],
  u'A00-2018': [ [],
                 [ u'1 Introduction Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.'],
                 []],
  u'A88-1019': [ [ u'So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags  and this again brings up the question of what to do about unseen or low-frequency forms.'],
                 [ u'In a related test, we explored the bracketings produced by Church&amp;apos;s PARTS program .'],
                 []],
  u'A92-1021': [ [u'Proven POS taggers predetermine POS features.'],
                 [ u' We use Brill?s tagger to POS-tag the English sentences, extract content words, and use WordNet'],
                 [ u"We restrict he test to senses within a single part of speech to focus the work on the harder part of the disambiguation problem--the accuracy of simple stochastic part-of-speech taggers uch as Brill's suggests that distinguishing among senses with different parts of speech can readily be accomplished."]],
  u'C92-2070': [ [ u'2 Genera l i z ing Lex ica l Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word .'],
                 [],
                 []],
  u'C92-2082': [ [ u'This is similar in nature to Hearst?s lexico-syntactic patterns and other approaches that derive ISA relations from text'],
                 [ u'Although we used the classical lexico-syntactic patterns for hyponymy acquisition to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone.'],
                 [ u'Another example is the use of patterns such as NP, NP * ,and otherNP to find lexical semantic information such as hyponym .']],
  u'C96-1058': [ [ u'As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank  although there are systems that extract labeled grammatical relations based on shallow parsing, e. and maximum spanning tree based dependency parsing .'],
                 [ u'They use a variant of Eisner?s generative model C for reranking and extend it to capture higher-order information than Eisner.'],
                 [ u'Currently, there are many dependency parsers available, such as Eisner?s probabilistic dependency parser  McDonald']],
  u'C96-2141': [ [],
                 [ u'The alignment aspect of our model is similar to the HMM model for word alignment .'],
                 [ u'Most current statistical models treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words']],
  u'E06-1011': [ [ u' 1 Introduction Higher-order dependency features encode more complex sub-parts of a dependency tree structure than first-order, bigram head-modifier relationships.1 The clear trend in dependency parsing has been that the addition of such higher-order features improves parse accuracy '],
                 [ u'Over the past few years, supervised learning methods have obtained state-of-the-art performance for dependency parsing '],
                 [ u'We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers .']],
  u'H05-1044': [ [ u'Instead of document level sentiment classification, analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees.'],
                 [],
                 [ u' The list of opinion words comes from the MPQA subjectivity lexicon .']],
  u'H05-1066': [ [ u'For example, setting ?1 = FIRST and ?t>1 = REST corresponds to first-order models, while setting ?1 = NULL and ?t>1 = xt?1 corresponds to sibling models .'],
                 [],
                 []],
  u'H94-1020': [ [],
                 [ u'7 Experiments We conducted two experiments on Penn Treebank II corpus .'],
                 []],
  u'J01-4004': [ [],
                 [ u'We started with the machine learning based 10 WordNet Wikipedia Prince Fela Kuti The Minneapolis Genius the pioneer of Afrobeat music The artist formerly known as Prince TAFKAP The Artist Raw text he Prince Fela Kuti the pioneer of Afrobeat music The Minneapolis Genius he TAFKAP The Artist The artist formerly known as Prince with coreference chains Text annotated Preprocessing pipeline PoS tagger Chunker NER Baseline Feature Extractor MaxEnt classifier Semantic Feature extractor SEMANTICS Figure 2: Overview of the coreference system for extrinsic evaluation of WordNet and Wikipedia relatedness measures.'],
                 []],
  u'J02-3001': [ [],
                 [ u'Our goal with these categories was to produce data useful for automatically labeling semantic roles  where selectional preferences play an important role.'],
                 [ u' 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents .']],
  u'J03-1002': [ [ u'We use GIZA++ to produce word alignments in Europarl: we ran it in both directions and kept the intersection of the induced word alignments.'],
                 [ u'The word alignment between the source input and reference is computed using GIZA++ beforehand with the default settings, then is refined with the heuristic grow-diag-finaland; the word alignment between the source input and the translation is generated by the decoder with the help of word alignment inside each phrase pair.'],
                 [ u'The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool on a parallel sentence aligned corpora']],
  u'J04-4002': [ [],
                 [ u'First, we identify initial phrase pairs using the same criterion as most phrase-based systems  namely, there must be at least one word inside one phrase aligned to a word inside the other, but no word inside one phrase can be aligned to a word outside the other phrase.'],
                 []],
  u'J05-1004': [ [ u'2.2 Semantic Role Labeling The task of semantic role labeling in the context of PropBank is to label tree nodes with semantic roles in a syntactic parse tree.'],
                 [],
                 [ u'but we can also include this treebank in the category of multistratal resources since the PropBank ']],
  u'J90-1003': [[], [], ''],
  u'J90-2002': [ [ u'2 Review: Noisy Channel Model The statistical translation model introduced by IBM views translation as a noisy channel process. '],
                 [ u'The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation .'],
                 [ u' The standard approach to word alignment is to construct directional generative models  which produce a sentence in one language given the sentence in another language']],
  u'J91-1002': [ [ u'In 1The term spatiM cohesion is motivated by the work on lexical cohesion by Morris and Hirst (.'],
                 [],
                 [ u' Sometimes segmentation is done at the clause level, for the purposes of detecting nuances of dialogue structure or for more sophisticated discourse-processing purposes ']],
  u'J92-4003': [ [],
                 [ u'3.1 Brown clustering The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams .anoop}@cs.sfu.ca Abstract We combine multiple word representations based on semantic clusters extracted from the .'],
                 []],
  u'J93-1003': [ [ u'We measured associations using the log-likelihood measure for each combination of target category and semantic class by converting each cell of the contingency into a 2.'],
                 [],
                 [ u'64 Table 1: Subjects of &amp;quot;employ&amp;quot; with highest likelihood ratio word freq logA word freq logA bRG 64 50.4 plant 14 31.0 company 27 28.6 operation 8 23.0 industry 9 14.6 firm 8 13.5 pirate 2 12.1 unit 9 9.32 shift 3 8.48 postal service 2 7.73 machine 3 6.56 corporation 3 6.47 manufacturer 3 6.21 insurance company 2 6.06 aerospace 2 5.81 memory device 1 5.79 department 3 5.55 foreign office 1 5.41 enterprise 2 5.39 pilot 2 5.37 *ORG includes all proper names recognized as organizations The logA column are their likelihood ratios .']],
  u'J93-1007': [ [ u' One of the best efforts to quantify the performance ofa term-recognition system does so only for one processing stage, leaving unassessed the text-to-output performance of the system'],
                 [],
                 [ u' Tools like Xtract were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output']],
  u'J93-2003': [ [ u'Most current statistical models treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words'],
                 [ u'1 Introduction Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research, for example,  including work leveraging syntactic parse trees, e. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based .'],
                 [ u'For the TM hit, the IBM Model 1 score serves as a rough estimation of how good a translation it is on the word level; for the MT output, on the other hand, it is a black-box feature to estimate translation quality when the information from the translation model is not available.']],
  u'J93-2004': [ [ u' Averaged Perceptron Algorithm 5 Experiments We evaluate our method on both Chinese and English syntactic parsing task with the standard division on Chinese Penn Treebank Version 5.0 and WSJ English Treebank 3.0 as shown in Table 1'],
                 [ u'We have chosen to work with a corpus with parse information, the Wall Street Journal WSJ part of the Penn Treebank II corpus  and to extract chunk information from the parse trees in this corpus.'],
                 [ u' As the parsing community trains on sections 2-21 of the Penn Treebank and tests on section 23, we create Gigaword sections by isolating specific months']],
  u'J94-2001': [ [ u' In the second one, the model is learned using the Baunl-\\?elch algorithm from an initial model which is estimated using labelled corpora '],
                 [ u' Eojeol 86.80 90.48 89.40 89.62 91.73 92.48 Morpheme 91.32 94.93 94.40 94.48 95.77 96.12 possible for training gives much better performance than unsupervised training using the Baum-Welch reestimation algorithm '],
                 [ u'EM was first used in POS tagging in which showed that except in conditions where there are no labeled training data at all, EM performs very poorly.']],
  u'J94-4002': [ [ u' A lot of work has been done in English for the purpose of anaphora resolution and various 81 algorithms have been devised for this purpose '],
                 [ u'3.3 Salience Score Filtering Previous work has reported the usefulness of salience for anaphora resolution .'],
                 []],
  u'J95-2003': [ [ u'However, such approaches tend to disregard theoretical findings from discourse theories, such as Centering Theory .'],
                 [ u' One popular model, the centering model  uses a ranking of discourse entities realized in particular sentences and computes transitions between adjacent sentences to provide insight in the felicity of texts'],
                 []],
  u'J95-4004': [[], '', ''],
  u'J96-1002': [ [],
                 [ u'As a model learning method, we adopt the maximum entropy model learning method .'],
                 [ u' In previous research on splitting sentences, many methods have been based on word-sequence characteristics like N-gram .']],
  u'J96-2004': [ [],
                 [ u'5 Reliability of Annotations 5.1 The Kappa Statistic To measure the reliability of annotations we used the Kappa statistic .'],
                 [ u'4.2 Interpreting reliability results It has been argued elsewhere that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test, reliability for category classifications should be measured using the kappa coefficient.']],
  u'J97-3002': [ [ u' However, we show that the dynamic program for a block ITG aligner can be augmented to score extraction sets that are indexed by underlying ITG word alignments '],
                 [ u'Since many concepts are expressed by idiomatic multiword expressions instead of single words, and different languages may realize the same concept using different numbers of words  word alignment based methods, which are highly dependent on the probability information at the lexical level, are not well suited for this type of translation.'],
                 []],
  u'J98-1004': [ [],
                 [],
                 [ u'as proposed in or in semisupervised settings, e. Among these approaches, the supervised corpus-based approach had been applied and discussed by many researches .']],
  u'M95-1005': [ [ u' Automatically annotated texts, produced by systems using the same markup scheme, were then compared with the manually annotated versions, using scoring software made available to MUC participants, based on '],
                 [],
                 [ u'As a supplement to the coreference class scoring scheme that was developed for the CO-task evaluation of the Message Understanding Conferences  two additional evaluation disciplines are defined that, on one hand, aim at supporting the developer of anaphor esolution systems, and, on the other hand, shed light on application aspects of pronoun interpretation.']],
  u'N03-1003': [ [ u' While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts have been restricted to at most two news sources.'],
                 [],
                 [ u' The latter study applied several MT metrics to paraphrase data from Barzilay and Lee?s corpus-based system  and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments']],
  u'N03-1017': [ [ u' The word alignment is computed using GIZA++ 2 for the selected 73,597 sentence pairs in the FBIS corpus in both directions and then combined using union and heuristic diagonal growing .'],
                 [ u'In  various aspects of phrase-based systems are compared, e. 2 The Problem of Coverage in SMT Statistical machine translation made considerable advances in translation quality with the introduction of phrase-based translation .'],
                 [ u'3.2.2 Alignment Error Rate Since MT systems are usually built on the union of the two sets of alignments  we consider the union of alignments in the two directions as well as those in each direction.']],
  u'N03-1020': [ [],
                 [ u' Another approach that is used for summarization evaluation is to use the ROUGE evaluation approach  which is based on n-gram co-occurrence, longest common subsequence and weighted longest common subsequence between the ideal summary and the extracted summary.'],
                 []],
  u'N03-1028': [ [ u'2 Conditional Random Fields We use a Conditional Random Field since it represents the state of the art in sequence modeling and has also been very effective at Named Entity Recognition.'],
                 [ u'especially those involving tagging words with labels such as partof-speech tagging and shallow parsing .'],
                 []],
  u'N03-1033': [ [ u' To select candidate terms, we first filter the improbable index terms 265 based on POS patterns using the Standard POS Tagger '],
                 [ u'260 All the extracted snippets are then processed: the text is tokenized and part-of-speech tagged using the Stanford tagger  and contexts that do not include the target word with the specified part-of-speech are removed.'],
                 [ u'We then obtain their POS N-grams from the Stanford POS tagger  and count how many of them have the POS N-gram p. To make use of the information about future tags, Toutanova et al proposed a tagging algorithm based on bidirectional dependency networks 467 and achieved the best accuracy on POS tagging on the Wall Street Journal corpus.']],
  u'N07-1051': [ [ u'1 Introduction Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank .'],
                 [ u'To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser trained on Chinese TreeBank 6.'],
                 [ u' 3.3 Coarse-to-fine Parsing Another common method employed to speed up exhaustive parsers is a coarse-to-fine approach, where a cheap, coarse model prunes the search space for later, more expensive models .']],
  u'P00-1056': [ [ u' 1 Introduction Word-aligned bilingual corpora provide important knowledge for many natural language processing tasks, such as the extraction of bilingual word or phrase lexica '],
                 [ u'3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards .'],
                 [ u'4.2 Training To train the translation model, we first run GIZA++ to obtain word alignment in both translation directions.']],
  u'P01-1067': [ [ u'mounting efforts have been directed towards SMT models employing linguistic syntax on the source side '],
                 [ u' Some systems attempt to model the differences directly  but most recent work focuses on reducing the sensitivity of the rule-extraction procedure to the constituency decisions made by 1-best syntactic parsers, either by using forest-based methods 863 The first step is to select team members S NP S VP VP VPTO VBZ ADVPVB  .'],
                 [ u'Recently, specific probabilistic tree-based models have been proposed not only for machine translation  but also for This work was supported by DARPA contract F49620-00- 1-0337 and ARDA contract MDA904-02-C-0450.']],
  u'P02-1014': [ [ u'5 Related work In recent years there has been a lot of work to develop anaphora resolution algorithms using both symbolic and statistical methods that could be quantitatively evaluated but this work focused on identity relations; bridging references were explicitly excluded from the MUC coreference task because of the problems with reliability discussed earlier.'],
                 [],
                 [ u' From the natural language perspective, there has been a lot of work on the related problem of coreference resolution - which aims at linking occurrences of noun phrases and pronouns within a document based on their appearance and local context.']],
  u'P02-1034': [ [],
                 [u'and boosting or perceptron approaches to reranking '],
                 []],
  u'P02-1038': [ [ u'4.1 Baseline For a baseline system we produced a phrase-based statistical machine translation system based on the log-linear formulation described in e. machine translation .'],
                 [ u' 5 The model Following  we adopt a general log-linear model.'],
                 []],
  u'P02-1040': [ [ u'Here, we don?t want to measure the semantic similarity between two answers, but just measure if two answers are similar to the word level, therefore, we apply BleuScore  which is a standard metric in machine translation for measuring the overlap between n-grams of two text fragments r and c. Case-insensitive BLEU4 was used as the evaluation metric.'],
                 [ u'This is confirmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU .'],
                 []],
  u'P02-1053': [ [ u' 2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal.'],
                 [ u' There exist two main approaches to the problem of extracting sentiment automatically.1 The lexicon-based approach involves calculating orientation for a document from the semantic orientation of words or phrases in the document '],
                 [u' as seen in learning methods for document-level SA .']],
  u'P03-1021': [ [ u'Decoding weights are optimized using Och?s algorithm to set weights for the four components of the loglinear model: language model, phrase translation model, distortion model, and word-length feature..2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval?2002 test sets using minimum error rate training .'],
                 [ u' to estimate one phrasebased and one hierarchical model for each corpus; log-linear weights were optimized by minimum error-rate training '],
                 [ u'We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT to tune the feature weights on the development data.']],
  u'P03-1054': [ [ u' The English sentences in the parallel corpus were prepro2The parallel corpus collected and other resources are all available in our website http://isoft.postech.ac.kr/?megaup/research/resources/ 567 cessed by the Stanford Parser 3 which provides a set of analyzed results including part-of-speech tag sequences, a dependency tree, and a constituent parse tree for a sentence.'],
                 [ u'For the English WSJ, high accuracy parsing models have been created, some of them using extensions to classical PCFG parsing such as lexicalization and markovization .'],
                 [ u' In our experiments, we obtain this combined representation from the output of the Stanford parser but any other broadly similar parser could be used instead.']],
  u'P04-1035': [ [ u'Secondly, we show that fine-grained opinion structural information can even be used to build features that improve a coarse-grained sentiment task: document polarity classification of reviews .'],
                 [ u'instance, when performing single-aspect sentiment analysis, the most relevant aspect of content structure is whether a given sentence is objective or subjective '],
                 []],
  u'P04-1054': [ [],
                 [ u'Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem .'],
                 [ u'So far, various supervised learning approaches have been explored in this field ']],
  u'P05-1012': [ [ u'1 Introduction Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora .'],
                 [ u' 1 Introduction In recent years, several dependency parsing algorithms have been proposed and achieved high parsing accuracies on several treebanks of different languages'],
                 [ u' 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 3 are able to represent non-projective structures  which is important when parsing free word order languages where discontinuous constituents are common']],
  u'P05-1022': [ [ u' We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing should be useful for predicting sentence fluency as well.'],
                 [ u' First, we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak?s reranking parser  and test them on section 23 with automatic parse trees.'],
                 []],
  u'P05-1033': [ [ u'The proposed composite language models are applied to the task of re-ranking the N-best list from Hiero  a state-of-the-art parsing-based machine translation system; we achieve significantly better translation quality measured by the Bleu score and .'],
                 [],
                 []],
  u'P05-1034': [ [],
                 [ u' or for re-ordering the output of translation models by statistical ordering models that access linguistic information on dependencies and part-of-speech '],
                 [ u'2.1 Dependency Orientation Based on the assumption that constituents generally move as a whole  we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word.']],
  u'P05-1045': [ [ u'2 Entity Type Classification Systems State-of-the-art tools for named entity recognition such as the Stanford NER Tagger .'],
                 [ u' First, if NPi is an NE, we create a feature whose value is the NE label of NPi, as determined by the Stanford CRF-based NE recognizer .'],
                 [ u'To create d we extract all NEs from the text using the Stanford NE Recognizer and represent each NE by its Wikipedia URI']],
  u'P05-1066': [ [ u'are statistically significant with p &lt; 0.05 using the sign test described in  compared to the baseline results in each table.'],
                 [],
                 []],
  u'P05-1074': [ [],
                 [ u'In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases Many data-driven approaches to the paraphrase problem have been proposed.'],
                 [ u'In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language .']],
  u'P06-1077': [[u'adopts a CKY style span-based decoding while '], [], []],
  u'P06-1121': [ [],
                 [ u'2 Related Work To model the syntactic transformation process, researchers in these fields?especially in machine translation?have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations .'],
                 []],
  u'P07-2045': [ [ u'4.2 Machine Translation The phrase-based translation experiments reported in this work was performed using the Moses2 toolkit for statistical machine translation.'],
                 [ u'Language Sentences Words Arabic 9,320,356 228,712,189 Chinese 2,039,399 49,564,193 Table 1: Training data: number of sentences and English words in the parallel training data 4 Experiment We trained translation models using Moses on the bilingual data provided by the LDC, with additional monolingual data from the English Gigaword corpus for an interpolated 5-gram language model.'],
                 [ u'For the SMT experiment, we use GIZA++ for the alignment between English and tokenized Arabic, and perform the translation using Moses phrasebased SMT system  with a maximum phrase length of 5.']],
  u'P90-1034': [ [],
                 [],
                 [ u'Syntactic context information is used to compute term similarities, based on which similar words to a particular word can directly be returned.']],
  u'P93-1024': [ [ u' Class based models distinguish between unobserved cooccurrences using classes of &amp;quot;similar&amp;quot; words'],
                 [ u'Distributional clustering assigns to each word a probability distribution over clusters to which it may belong, and characterizes each cluster by a centroid, which is an average of cooccurrence distributions of words weighted according to cluster membership robabilities.'],
                 [ u' This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles ']],
  u'P95-1026': [ [ u' A hybrid class of machine learning techniques for WSD relies on a small set of hand labeled data used to bootstrap a larger corpus of training data '],
                 [ u'Most recently, Yarowsky used an unsupervised learning procedure to perform WSD  although this is only tested on disambiguating words into binary, coarse sense distinction.. Yarowsky proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples.'],
                 [ u' Another related area to this work is the field of word sense induction: the task of identifying the possible senses of a word in a corpus using unsupervised methods  as opposed to traditional disambiguation methods which rely on the availability of a finite and static list of possible meanings.']],
  u'P95-1037': [ [],
                 [],
                 [ u'The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn treebank using head percolation rules used in  which do not contain any order information.']],
  u'P96-1025': [ [ u'On the other hand, as large-scale tagged corpora have become available these days, a number of statistical parsing techniques which estimate the dependency probabilities using such tagged corpora have been developed.'],
                 [],
                 []],
  u'P96-1041': [ [ u'The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing using only English sentences in the parallel training data.'],
                 [],
                 []],
  u'P97-1003': [[], [], []],
  u'P97-1023': [ [ u'2.3 Contextual Word Polarity Detection Much work on sentiment analysis have been directed to determine the polarity of opinion using anotated lexicons with prior polarity .'],
                 [ u'Many previous systems studied the problem of iden133 tifying the polarity of individual words '],
                 [ u'Work by Hatzivassiloglou and colleagues used clustering methods to automatically identify adjectival scales from corpora.']],
  u'P98-1013': [ [ u' PropBank is organized in two layers, the first one being an underspecified representation of a sentence with numbered arguments, the second one containing fine-grained information about the semantic frames for the predicate comparable to FrameNet '],
                 [],
                 [ u'1 Introduction The availability of large scale semantic lexicons, such as FrameNet  allowed the adoption of a wide family of learning paradigms in the automation of semantic parsing.']],
  u'P98-2127': [ [ u'Research in distributional similarity has found that entries in distributional thesauri tend to also contain terms that are opposite in meaning .'],
                 [ u'5.2 Lexical Clustering In order to address the sparse coverage of lexical head word statistics, an experiment was carried out using an automatic clustering of head words of the type described in .'],
                 []],
  u'P99-1067': [ [ u'Previous work has sought to learn new translations for words by looking at comparable, but not parallel, corpora in multiple languages and analyzing the cooccurrence of words, resulting in the generation of new word-to-word translations.'],
                 [],
                 [ u'Comparable documents can also be used for learning word-level translation lexicons .']],
  u'W02-1001': [ [ u'8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm .'],
                 [],
                 [ u'2 Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm is a general learning algorithm.']],
  u'W02-1011': [ [ u'1 Introduction Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains  e. Machine learning techniques have been proposed for sentiment classification based on annotated samples from experts, but they have limited 1A feature broadly means an attribute or a function of a service.'],
                 [ u' Feature sets UG, BG, POS U, POS B are common features for sentiment analysis '],
                 []],
  u'W02-1018': [[], '', ''],
  u'W03-0419': [[], [], ''],
  u'W04-1013': [ [ u'1 Introduction ROUGE is a suite of automatic evaluations for summarization and was introduced a decade ago as a reasonable substitute for costly and slow human evaluation'],
                 [ u'Automatic evaluation was performed with ROUGE  a widely used and recognized automated summarization evaluation method.'],
                 [ u'We also used the automatic evaluation metric ROUGE as a rough estimate of summary recall: this metric computes the percentage of n-grams in the reference text that appeared in the generated summaries.']],
  u'W04-3250': [ [ u'Statistical significance in BLEU score differences was tested by paired bootstrap re-sampling .'],
                 [ u'We perform a bootstrap resampling significance test on the output predictions of the local classifiers with and without the inference model.. Statistical significance tests are performed using the paired bootstrap resampling method .'],
                 [ u'The improvement is statistically significant with 95% confidence using pairwise bootstrapping of 1,000 test sets randomly sampled with replacement .-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34 GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67 ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71 Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasal ITG models for word and character-based translation, with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05 .']],
  u'W05-0620': [ [],
                 [ u' In the CoNLL-2005 shared task on SRL  however, a task of training and evaluating systems on different domains was included'],
                 [ u'Semantic role labeling based and rule based techniques can be employed to extract this target as topic word.']],
  u'W05-0909': [ [],
                 [ u'Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality and is simpler to measure.'],
                 []],
  u'W06-2920': [ [ u'We ascribe the widespread interest in dependency structures to their intuitive appeal, their conceptual simplicity, and in particular to the availability of accurate and efficient dependency parsers for a wide range of languages .'],
                 [ u'Some support for this view can be found in the results from the CoNLL shared tasks on dependency parsing in 2006 and 2007, where a variety of data-driven methods for dependency parsing have been applied with encouraging results to languages of great typological diversity .'],
                 [ u'For experiments in Bulgarian, German, Japanese, and Spanish, we use the CONLL-X data set with training data taken from the official training files.']],
  u'W95-0107': [ [],
                 [ u'For extracting simple noun phrases we first used Ramshaw and Marcus?s base NP chunker .'],
                 [ u'The last two are ideographic full-stop and comma.2 4 Unsupervised partial parsing We learn partial parsers as constrained sequence models over tags encoding local constituent structure ']],
  u'W96-0213': [ [],
                 [],
                 [ u'We used the MXPOST tagger trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus.']],
  u'W99-0604': [ [ u' 2.1 Phrase Based MT Phrase-based methods identify contiguous bilingual phrase pairs based on automatically generated word alignments .'],
                 [ u'1 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models .'],
                 []],
  u'W99-0613': [ [ u' This family of techniques has met with success in semisupervised named entity classification .'],
                 [ u'Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set .'],
                 [u'directed NER research .']]}
